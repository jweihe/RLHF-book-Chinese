---
prev-chapter: "推理训练与推理时扩展"
prev-url: "14-reasoning.html"
page-title: 合成数据与蒸馏
next-chapter: "评测与提示工程"
next-url: "16-evaluation.html"
---

# 合成数据与蒸馏

人类反馈强化学习（RLHF）最初的核心理念，是让人类对模型训练产生直接影响。
在RLHF早期，想要提升模型能力，几乎只能依赖人类数据。

只有人类，才能为问题生成足够高质量的回答用于训练；
只有人类，才能收集到可靠且细致的反馈数据来训练奖励模型。

但随着AI模型能力的提升，这一假设很快被打破。
合成数据的出现——其成本更低、迭代更快——让RLHF从唯一焦点转变为更广义的“后训练”，合成数据成为模型塑造的关键工具。

虽然有不少研究指出合成数据会导致“模型坍塌”或其他性能问题 [@shumailov2024ai]，但主流大模型的实际表现已有力反驳了这些担忧 [@gerstgrasser2024model] [@feng2024beyond]。
合成数据*确实可能*导致模型性能下降，但这通常是因为数据高度重复，或仅用模型自身输出（导致分布收窄），而不是数据来源多样、筛选得当时的问题。

顶尖大模型**离不开合成数据**才能达到最佳性能。
现代后训练中的合成数据涉及多个环节——用语言模型从种子样本生成新训练prompt [@wang2022self]，修改已有prompt，生成prompt的补全 [@numina_math_7b]，用AI反馈生成偏好数据 [@cui2023ultrafeedback]，用AI过滤补全 [@li2024superfiltering]，等等。
可以说，合成数据已成为后训练的核心。

合成数据之所以能产生如此深远的影响，是因为GPT-4级别模型的出现。
早期大模型（如Llama 2、GPT-3.5-Turbo）还无法稳定地生成或监督数据流程。
但仅仅1-2年后，语言模型在生成答案方面已远超人类。
从GPT-3.5到GPT-4的升级，也让“LLM裁判”（LLM-as-a-judge）成为现实。
GPT-4及更强模型在生成反馈或评分时表现得更加稳健、一致。

自此之后，合成数据在大模型训练中的作用只增不减。
不过，人类数据在两个方面依然不可替代：

1. 在模型尚未具备相关能力的边缘领域，仍需人类生成数据。一旦有第一个强模型出现，合成数据便会迅速扩散。
2. 即使学术研究显示合成偏好数据表现同样优秀，主流大模型依然会用到人类偏好数据。人类偏好的真正作用在文献中仍在不断被探讨。

“蒸馏”（distillation）一词，是讨论合成数据在大模型中的最重要概念之一。
蒸馏最早来源于深度学习中的“教师-学生知识蒸馏”技术定义 [@hinton2015distilling]。

在业界口语中，蒸馏泛指用更强模型的输出来训练更小的模型。
在后训练阶段，蒸馏主要有两种常见形式：

1. 作为数据引擎，贯穿后训练流程：用于生成指令补全、偏好数据（或宪法AI）、或RL的验证数据。
2. 将特定技能从强模型迁移到弱模型，常见于数学推理、代码等专项能力。

第一种方式随着大模型能力超越人类而愈发流行。
GPT-4级模型让蒸馏能用于复杂任务（如数学、编程等）。
在企业内部，常见做法是先训练一个大型闭源模型（如Claude Opus、Gemini Ultra），内部用来生成更强的下游模型。
在开源生态中，常见做法是将API模型的训练数据蒸馏到小型开源模型 [@tunstall2023zephyr]。
在这一过程中，精心设计高质量prompt、对教师模型输出进行筛选，是提升最终效果的关键。

将专项技能迁移到小模型，同样遵循蒸馏原则——获取最优训练数据。
近年来，许多论文研究用强模型的小规模数据集提升小模型的对齐能力
[@zhou2023lima]、数学推理 [@shridhar2023distilling] [@hsieh2023distilling]、
以及测试时扩展能力 [@muennighoff2025s1]。