---
prev-chapter: "风格与信息"
prev-url: "18-style.html"
page-title: 产品、用户体验与模型个性
next-chapter: ""
next-url: ""
---

# 产品、用户体验与模型个性

RLHF与后训练领域的前沿进展，展现了这些技术在公司内部如何被用于打造领先的AI产品。  
随着RLHF日益成熟，它所解决的问题也变得更加细腻和多元。
本章将讨论一系列主流AI实验室在产品落地中考虑、但学术文献鲜有深入探讨的RLHF/后训练应用场景。

## 角色训练（Character Training）

角色训练（Character Training）是后训练中的一个分支，关注于塑造模型的“性格”或风格，而非单纯内容本身。
角色训练对语言模型聊天机器人的用户体验极为重要，但在公开领域几乎没有系统研究。

目前，我们对角色训练的权衡、评测方法、对ChatBotArena等评测的提升程度等问题都知之甚少，但这些问题值得深入探索。
我们*已知*的是，角色训练会用到本书介绍的同类技术，只是目标更精细地聚焦于模型输出语言的风格特征。
角色训练通常涉及大量数据筛选和合成数据方法（如宪法AI），重点在于模型行为方式的塑造。
这些变化往往难以在传统评测体系中量化，因为实验室会用角色训练对模型性格做小幅度、渐进式调整，以持续优化用户体验。

举例来说，Anthropic在Claude 3模型中引入了角色训练 [@anthropic2024claude]：

> Claude 3 是我们首次在对齐微调流程中引入“角色训练”的模型：这是初步训练结束后，将模型从预测文本的工具转变为AI助手的阶段。角色训练的目标，是让Claude具备更细腻、更丰富的特质，比如好奇心、开放性和思考力。

随后数月，行业内各家模型的“性格”都变得更加鲜明。
这一过程极度依赖合成数据，但如官方博客所说，也需要“艺术家的手感”，即人类研究者密切观察每项性格特质对模型行为的影响。

角色训练成为行业焦点，正说明RLHF及相关技术已从哲学上的“对齐”转向以经验为主的实用工具。模型能捕捉多样行为，但让其稳定地表现出我们想要的风格，是最难的部分。如今，更像是在追求RLHF作为性能工具的上限，而非仅仅是安全工具。

关于角色训练，少数公开讨论来自Anthropic的Amanda Askell在Lex Fridman播客上的访谈（节选自文字稿）：

> Lex Fridman（03:41:56）：你说的角色训练具体包含什么？是RLHF还是别的什么？
>
> Amanda Askell（03:42:02）：更像是宪法AI，是那条技术线的变体。我会设计模型应该具备的性格特质，可以是简短的，也可以是更详细的描述。然后让模型生成与这些特质相关的用户问题，再生成回复，并根据性格特质对回复进行排序。在生成query之后，这一流程与宪法AI非常类似，但也有些不同。我很喜欢这种方式，因为这就像Claude在训练自己的性格，不涉及任何人类数据……本质上就是宪法AI，只是没有用到人工数据。

总之，Anthropic用宪法AI和通用后训练的技术，来训练模型的“个性”。

## 模型规范（Model Specifications）

OpenAI最近公开了所谓的“模型规范”（Model Spec）[@openai2024modelspec]，这是一份在微调前明确记录模型目标行为的文档。
它不仅关乎模型本身，还涉及OpenAI如何在API背后引导模型行为，以及未来模型将如何演进。

Model Spec是业界和RLHF中为数不多的、能将模型实际行为与设计者意图进行对比的工具。
正如本书多次提到的，模型训练是复杂且多元的过程，最终结果难免与数据标注说明、训练任务分布等初衷有偏差。
与宪法AI等原则列表相比，Model Spec更直接体现了设计意图，而不是仅仅列出中间训练变量。

Model Spec能为模型发布流程中的各方带来价值：

- **模型设计者**：有助于明确希望/不希望模型具备哪些行为，便于数据优先级决策，聚焦长期方向之外的重点，全面审视模型在复杂评测体系中的定位。
- **开发者**：模型用户能更清楚哪些行为是有意为之（如某些拒绝），哪些是训练副作用，从而更有信心采用未来更智能的模型。
- **公众**：Model Spec为外界了解训练优先级提供了少有的窗口，对监管和制定AI政策具有重要意义。

## 产品周期、用户体验与RLHF

随着强大AI模型越来越像产品而不是单纯的机器学习实验品，RLHF成了模型与产品之间的接口。
让模型易用，远不止权重正确——还包括推理速度、工具集成（如搜索、代码执行）、可靠且易用的用户界面（UX）等。
RLHF研究已成为检验这些要素的接口，因为它能实时反映用户对产品的偏好，也是模型上线前的最后训练环节。
最快为模型添加新特性的方法，就是在后训练阶段尝试集成——这阶段训练更快、更便宜。
这一循环已在图像理解、工具调用、行为优化等方向反复上演。
许多产品需求最终会转化为RLHF建模问题，一旦在这里取得成功，还会反向影响更早的训练阶段。